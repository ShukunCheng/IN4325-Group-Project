{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTerrier\n",
    "\n",
    "_IN4325: Information retrieval lecture, TU Delft_\n",
    "\n",
    "**Part 5: Transformers**\n",
    "\n",
    "This notebook introduces PyTerrier _transformers_ (not to be confused with [neural transformer models](<https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)>)). We'll learn about the different types of data frames that PyTerrier uses and how the transformers operate on them.\n",
    "\n",
    "In order to run everything in this notebook, you'll need [pyspellchecker](https://github.com/barrust/pyspellchecker) installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install python-terrier pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're going to use RM3 in this notebook, we'll need to load the `terrier-prf` plugin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(\n",
    "        tqdm=\"notebook\", boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we'll illustrate the different kinds of transformers and data frames using examples. Note that we're only scratching the surface here, so **make sure to have a look at the [documentation](https://pyterrier.readthedocs.io/)**!\n",
    "\n",
    "We'll use the `nfcorpus` dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"irds:nfcorpus/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we'll need an index with blocks (i.e., positional information), so we need to create a new one. Since memory indexes do not support blocks at the moment, we'll create one on disk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "idx_path = Path(\"nfcorpus_index_with_blocks\").absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index the corpus with `blocks=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.index.IterDictIndexer(\n",
    "    str(idx_path),\n",
    "    blocks=True,\n",
    ").index(dataset.get_corpus_iter(), fields=[\"title\", \"abstract\", \"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "Recall that the queries (topics) of a dataset can be accessed using the `get_topics` method. For this dataset, there are multiple variants; we choose `title`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = dataset.get_topics(variant=\"title\")\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, PyTerrier represents all data as `pandas.DataFrame` objects.\n",
    "\n",
    "The method above outputs a data frame with two columns, `qid` and `query`. In PyTerrier, data frames of this format are referred to as _data type_ `Q`, and they essentially represent a set of queries, each of which has a unique identifier. In fact, we have already constructed our own `Q` data frames in the scaffolding project.\n",
    "\n",
    "There are some other data types, and we will introduce them throughout the rest of this series. You can find an overview [here](https://pyterrier.readthedocs.io/en/latest/datamodel.html).\n",
    "\n",
    "## Transformers\n",
    "\n",
    "_Transformers_ directly operate on these data frames; in other words, a transformer takes as input a data frame of some type and outputs another data frame (of the same or another type). We'll take a look at several pre-implemented transformers in this notebook.\n",
    "\n",
    "### Retrieval transformers\n",
    "\n",
    "Retrievers are the most common transformers, and we have already used them plenty throughout this introductory series. For example, let's take a BM25 model as before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pt.IndexFactory.of(str(idx_path))\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformer consumes data of type `Q` and returns data of type `R` (i.e., columns `qid`, `docno`, `score`, `rank`), which corresponds to a ranking. The transformation can be invoked by calling the `transform` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25.transform(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our result is actually a superset of `R` (we have an additional column, `query`). In general, data frames may have more columns than a specific transformer requires, but they can still be used.\n",
    "\n",
    "Alternatively, the transformer can be called directly, as we have done so far, which gives the same result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, transformers implement the `search` method, which processes a single query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25.search(\"what is the meaning of life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query rewriting transformers\n",
    "\n",
    "You have already experimented with query rewriting in the scaffolding project. PyTerrier implements several transformers that rewrite queries.\n",
    "\n",
    "The simplest one is the _sequential dependence model_:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdm = pt.rewrite.SequentialDependence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDM requires positional information in the index (that's why we needed to set a flag during indexing). More information about SDM can be found [here](https://pyterrier.readthedocs.io/en/latest/rewrite.html#sequentialdependence).\n",
    "\n",
    "It operates solely on the queries themselves; in other words, both input and output are data frames of type `Q`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdm(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `query` column contains the new (rewritten) queries, while the original queries are retained in the `query_0` column.\n",
    "\n",
    "#### Query expansion\n",
    "\n",
    "_Query expansion_ differs from standard query rewriting in that it operates on queries **and** corresponding relevant documents (these need to be retrieved based on the original queries prior to the expansion). This is also known as _pseudo relevance feedback_ (PRF). A popular PRF model is _RM3_:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm3 = pt.rewrite.RM3(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since RM3 requires a set of documents for each query, its input type needs to be `R`. Consequently, we can use the result of our retriever as an input for the PRF model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm3(bm25(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "You probably noticed that the transformers we've seen so far are mostly designed to work in sequence; for example, reformulating queries alone is pointless without an actual retrieval step afterwards.\n",
    "\n",
    "This is where _pipelines_ come into play. PyTerrier implements the `>>` operator to build sequences of transformers. Let's build a simple pipeline that applies SDM and then retrieves documents using BM25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_sdm = sdm >> bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this pipeline like any other transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_sdm(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare SDM and RM3 in terms of performance.\n",
    "\n",
    "First, we create a pipeline for RM3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_rm3 = bm25 >> rm3 >> bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run an experiment to evaluate and compare both of these pipelines. We'll also include standalone BM25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import MAP, nDCG\n",
    "\n",
    "pt.Experiment(\n",
    "    [bm25, pl_sdm, pl_rm3],\n",
    "    queries,\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"BM25\", \"SDM >> BM25\", \"BM25 >> RM3 >> BM25\"],\n",
    "    eval_metrics=[MAP, nDCG @ 10],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "There are a number of _operators_ that can be applied to transformers within pipelines. We've already seen the `>>` operator. Here, we'll look at a few more selected operators. You can find the complete list [here](https://pyterrier.readthedocs.io/en/latest/operators.html).\n",
    "\n",
    "#### Rank cutoff\n",
    "\n",
    "The `%` operator limits how many documents per query are kept (the lowest-scoring ones are removed). For example, we may want to consider only a single document for RM3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_rm3_1doc = (bm25 % 1) >> rm3 >> bm25\n",
    "\n",
    "pt.Experiment(\n",
    "    [pl_rm3, pl_rm3_1doc],\n",
    "    queries,\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"RM3\", \"RM3 (1 document)\"],\n",
    "    eval_metrics=[MAP, nDCG @ 10],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching\n",
    "\n",
    "The `~` operator can be used to automatically cache the output of a transformer. Let's time our BM25 retriever without caching first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit bm25(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we enable caching. This should make it much faster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit (~bm25)(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: When you use caching, make sure to clear the cache when you make changes to the transformers you cached. Otherwise, you might get unexpected results. The default location of the cache is `~/.pyterrier/transformer_cache/` (on Linux and macOS systems).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining rankings\n",
    "\n",
    "The `+` and `*` operators can be used to linearly combine two transformers that output rankings (data type `R`). For example, we can use two different retrievers and combine them as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "pt.Experiment(\n",
    "    [tf_idf, bm25, 2 * tf_idf + bm25],\n",
    "    queries,\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"TF-IDF\", \"BM25\", \"2 * TF-IDF + BM25\"],\n",
    "    eval_metrics=[MAP, nDCG @ 10],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the operations are applied to the scores computed by the retrievers. If a document is missing for one of the retrievers, a score of `0` is used.\n",
    "\n",
    "### Compiling pipelines\n",
    "\n",
    "Pipelines can be compiled. The compilation may (or may not) improve the efficiency for certain operations. For example, consider the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = bm25 % 3\n",
    "pl_compiled = pl.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time them both:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pl(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pl_compiled(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers\n",
    "\n",
    "PyTerrier makes it easy for you to implement your own custom transformers. In fact, we've used a custom query transformer under the hood for the scaffolding project.\n",
    "\n",
    "### `apply` functions\n",
    "\n",
    "[`pyterrier.apply`](https://pyterrier.readthedocs.io/en/latest/apply.html) allows for applying a custom function to each row of a data frame. There are many `apply` functions, each of which focuses on different data types. An overview can be found [here](https://pyterrier.readthedocs.io/en/latest/apply.html#module-pyterrier.apply).\n",
    "\n",
    "Let's implement one that reformulates the query to sound a bit nicer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_nicely = pt.apply.query(\n",
    "    lambda row: \"please find some information about \" + row[\"query\"]\n",
    ")\n",
    "ask_nicely(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending `pyterrier.Transformer`\n",
    "\n",
    "More complex transformers can be implemented by extending the base class directly.\n",
    "\n",
    "Here, we implement a transformer that naively corrects supposed spelling mistakes using a spell checking library. In order to do this, we only need to implement the `transform` method. We adapt the behavior of the other query rewriters and retain the original formulation in the `query_0` column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "class CorrectQuerySpelling(pt.Transformer):\n",
    "    def __init__(self):\n",
    "        self.spellchecker = SpellChecker()\n",
    "        super().__init__()\n",
    "\n",
    "    def _correct_spelling(self, query: str) -> str:\n",
    "        result = []\n",
    "        for word in query.split(\" \"):\n",
    "            if len(self.spellchecker.unknown([word])) > 0:\n",
    "                result.append(self.spellchecker.correction(word) or word)\n",
    "            else:\n",
    "                result.append(word)\n",
    "        return \" \".join(result)\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_new = df.copy()\n",
    "        df_new[\"query_0\"] = df_new[\"query\"]\n",
    "        df_new[\"query\"] = df_new[\"query_0\"].map(self._correct_spelling)\n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_query_spelling = CorrectQuerySpelling()\n",
    "correct_query_spelling(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "Check out the sections about the [data model](https://pyterrier.readthedocs.io/en/latest/datamodel.html) and [transformers](https://pyterrier.readthedocs.io/en/latest/transformer.html) in the PyTerrier documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
