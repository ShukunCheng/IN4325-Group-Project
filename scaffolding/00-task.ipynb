{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaffolding project\n",
    "\n",
    "_IN4325: Information retrieval lecture, TU Delft_\n",
    "\n",
    "Welcome to the **IN4325: Information retrieval** lecture!\n",
    "\n",
    "This project acts as a gentle introduction to information retrieval for you. You do not need any prior knowledge about IR for this task. Only some Python programming skills are required.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Under the hood, this notebook uses a library called **PyTerrier**. Please check out the first part of our _Introduction to PyTerrier_ series to learn how to install PyTerrier. However, you do not need to interact with PyTerrier directly for now; rather, we're providing you with simple utility functions you can use. Feel free to have a look how these are implemented, but it's not required.\n",
    "\n",
    "**Task 1**: Install PyTerrier (see the `01-setup.ipynb` notebook).\n",
    "\n",
    "Now you should be able to import the utility functions. The first time you do this, a dataset will be downloaded and indexed automatically (this will take a minute). If you have any issues running this cell, try removing the `index` directory (if it exists) and restarting the kernel of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import search, evaluate, evaluate_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, you can run search queries. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"what is the meaning of life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you get here is a list of ten documents from the corpus that are ordered by how relevant they are to our query (according to the search engine).\n",
    "\n",
    "## Query rewriting\n",
    "\n",
    "The goal of this task is to come up with a way of **rewriting queries** such that the search engine can \"understand\" them better.\n",
    "\n",
    "In order to do this, let's first take a look at some example queries from our dataset. We represent these queries using a `pandas.DataFrame`, where the first column corresponds to the **query ID** and the second column corresponds to the **query**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "example_queries = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            \"443848\",\n",
    "            \"does anybody know where i could get a free guide on how to train a siberian husky\",\n",
    "        ],\n",
    "        [\n",
    "            \"1783010\",\n",
    "            \"what is blaphsemy\",\n",
    "        ],\n",
    "        [\n",
    "            \"2838988\",\n",
    "            \"how can i get a cork out of not into a wine bottle without a corkscrew\",\n",
    "        ],\n",
    "    ],\n",
    "    columns=[\"qid\", \"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these queries are taken from the dataset, we can **evaluate the performance** of our search engine on these queries. This means that we know which documents the system should retrieve for each query.\n",
    "\n",
    "You can use the following evaluation function to do this. This function takes your queries and returns a score (mean average precision -- you will learn about this later). For now, all you need to know is that, the higher this score, the better the system works.\n",
    "\n",
    "Let's evaluate the queries we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"score:\", evaluate(example_queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's up to you to figure out if and how it's possible to make the search engine perform better on these queries. How would you query a search engine if you wanted to know about these topics? Experiment a bit.\n",
    "\n",
    "**Task 2**: Try to manually come up with ways to rewrite or reformulate the queries so the performance improves.\n",
    "\n",
    "**Important**: Make sure that the query IDs match! Otherwise, evaluation will not work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_queries_rewritten = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            \"443848\",\n",
    "            \"\",  # TODO: add rewritten query here\n",
    "        ],\n",
    "        [\n",
    "            \"1783010\",\n",
    "            \"\",  # TODO: add rewritten query here\n",
    "        ],\n",
    "        [\n",
    "            \"2838988\",\n",
    "            \"\",  # TODO: add rewritten query here\n",
    "        ],\n",
    "    ],\n",
    "    columns=[\"qid\", \"query\"],\n",
    ")\n",
    "print(\"score after rewriting:\", evaluate(example_queries_rewritten))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An automatic approach\n",
    "\n",
    "In this last part, we'll try to come up with an automatic approach to perform query re-writing. Use your findings from task 2 for this.\n",
    "\n",
    "**Task 3**: Implement a function that automatically re-writes any input query.\n",
    "\n",
    "You can use any approach or library you want for this task. However, keep in mind that simple ideas often work well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(query: str) -> str:\n",
    "    # TODO: rewrite the query here\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we'll evalute on _all_ queries in the dataset. This will give us a more general result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"score:\", evaluate_all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to improve the overall performance using your rewriting approach?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"score after rewriting\", evaluate_all(rewrite_query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
